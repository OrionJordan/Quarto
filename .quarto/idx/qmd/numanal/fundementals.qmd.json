{"title":"Fundementals","markdown":{"yaml":{"title":"Fundementals","format":{"html":{"toc":true,"html-math-method":"katex"}}},"headingText":"Floating point","containsRefs":false,"markdown":"\n\n\nComputers don't use exact numbers when preforming calculations, but instead use a floating point system. \n\n::: {.callout-note icon=\"false\"}\n## Definition\n\nA [**floating point**]{style=\"color: blue\"} number $x$ is expressed as \n\n$$\nx = S \\times 2^E \\times 1.b_1b_2\\ldots b_{52}\n$$\n\nSuch that $S$ represents the sign of the number, $+$ or $-$. $E$ is an exponent, and $b_1$ through $b_52$ are binary digits.\n\n:::\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nSuppose we have a variable $x = 6$ and we wish to express it as a floating point number.  We first convert the number to binary\n\n$$\n6_{10} = 110_{2}\n$$\n\nWe now specify where the decimal is, by adding an exponent \n\n$$\n6_{10} = 110_{2} = 2^2\\times1.10_2\n$$\n\nAnd finally adding the sign\n\n$$\n6_{10} = 110_{2} = 1\\times2^2\\times1.10_2\n$$\n\n:::\n\nWe can also express fractions, particular ones without a terminating digit. For example,\n\n$$\n0.3 = 1 \\times 2^{-2} \\times 1.00110011001\\ldots\n$$\n\nTerminating after $52$ digits.\n\n### Loss of signifigance\n\nComputers themselves store numbers as a collection of $64$ bits\n\n$$\nx = s_1e_1\\ldots e_{11}m_1\\ldots m_{52}\n$$\n\nOne bit for the sign of a number, $11$ bits for the exponent, and $52$ bits for the mantissa (digits after the decimal place).\n\nClearly this means that only a finite amount of numbers can be expressed in floating point, and numbers are rounded to their nearest corresponding floating point. This leads to a [**loss of signifigance**]{style=\"color: blue\"}, and means functions with extremly large or small inputs will be catastrophically wrong, as the difference between two numbers is completly outside of the range floating point can represent.\n\nTake the identical functions $f(x)$ and $g(x)$\n\n\n$$\nf(x) = \\frac{1 - \\cos(x)}{\\sin^2(x)}\n$$\n\n$$\ng(x) = \\frac{1}{1 + \\cos(x)} \n$$\n\n$f(x)$ suffers from an egregious loss of signifigance error as $x$ decreases towards $0$, But $g(x)$ won't.\n\n### Rounding\n\nThe number $101.101$ rounds up to $110$, because the digits after the decimal are above $.1$. If they aren't the number terminates at the decimal. If $101.1\\overline{0}$ then we get $110$, but if we have $110.1\\overline{0}$, we drop the number. This is based on the digit in the units place. If it is a $1$, we round up, if it is not, we drop it.\n\n## Taylor series\n\nThe [**taylor expansion**]{style=\"color: blue\"} of a function $f(x)$ is given by\n\n$$\nf(x) = \\sum^m_{k=0} \\frac{f^{(k)}(x)(x-x_0)^k}{k!} + \\frac{f^{(m + 1)}(c)(x-x_0)^{m+1}}{(m+1)!}\n$$\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nTake the function $f(x) = e^x$ We can taylor expand it to get\n\n$$\nf(x) = e^x = 1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\cdots + \\frac{x^k}{k!} + \\frac{e^cx^{k+1}}{k+1!}\n$$\n:::\n\nWe can also write the Taylor series\n\n$$\nf(x + h) = \\sum^m_{k=0} \\frac{h^kf^{(k)}(x)}{k!}\n$$\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nFind the error\n\n\\begin{align*}\ny^\\prime(x) &= ay(x) + by(x + h) + \\varepsilon\\\\\ny^\\prime(x) &= ay(x) + b\\left(y(x) - hy^\\prime(x) + \\frac{h^2}{2}y^{\\prime\\prime}(c_1)\\right) + \\varepsilon\\\\\ny^\\prime(x) &= (a+b)y(x) - bhy^\\prime(x) + \\frac{bh^2}{2}y^{\\prime\\prime}(c_1) + \\varepsilon\\\\\n\\end{align*}\n\nSo \n\n$$\na = -b\\quad b = -\\frac{1}{h}\\quad \n$$\n\nThus we get\n\n$$\ny^\\prime(x) = \\frac{y(x+h) - y(x)}{h} + \\varepsilon\n$$\n\nwith an error\n$$\n\\varepsilon = -\\frac{h}{2}y^{\\prime\\prime}(c_1).\n$$\n:::\n\nWhen we plug these numbers into the computer however, we are going to have an error as we have only $52$ digits of precision. This gives us the machine error $\\epsilon$. So if we wanted to numerically approximate the derivative\n\n$$\ny^\\prime(x) = \\frac{y(x+h) - y(x) + 2\\epsilon y}{h} + \\varepsilon\n$$\n\nThis gives us the total error, the sum of the machine error, and the taylor error.\n\n$$\n\\varepsilon_T = \\epsilon + \\varepsilon\n$$\n\n$$\n\\varepsilon_T = \\frac{2\\epsilon |y|}{2h} + \\frac{2h^2}{12}|y^{\\prime\\prime\\prime}(x)|\n$$\n\nBy computing the derivative, we can find the smallest total error.\n\n$$\n\\frac{d\\varepsilon_T}{dh} = -\\frac{1}{h^2}\\epsilon |y| + \\frac{h}{3}|y^{\\prime\\prime\\prime}| = 0\n$$\n\nSolving for $h$\n\n$$\nh_{opt} =\\left(\\frac{3\\epsilon |y|}{|y^{\\prime\\prime\\prime}|}\\right)^\\frac{1}{3}\n$$\n\nFor example, lets try the function $y = e^x$. This gives us an optimal error\n\n$$\nh_{opt} = \\left( 3 \\times 2^{-52} \\frac{e^x}{e^x} \\right)^\\frac{1}{3}\n$$\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nLets find the optimal value of $h$ using a different error.\n\n$$\n\\varepsilon = -\\frac{h}{2}y^{\\prime\\prime}(c_1)\n$$\n\nWe can rewrite our equation\n\n$$\ny^\\prime(x) = \\frac{y(x+h) - y(x) + 2\\epsilon y}{h} + -\\frac{h}{2}y^{\\prime\\prime}(c_1)\n$$\n\nThen\n\n$$\n\\varepsilon_T = \\frac{2\\epsilon |y|}{h} + \\frac{h}{2}y^{\\prime\\prime}\n$$\n\nTaking the derivative\n\n$$\n\\frac{d \\varepsilon_T}{dh} = -\\frac{2\\epsilon |y|}{h^2} + \\frac{1}{2}y^{\\prime\\prime} = 0\n$$\n\nWe get\n\n$$\n\\frac{2\\epsilon |y|}{h^2} = \\frac{1}{2}y^{\\prime\\prime}\n$$\n\nThus\n\n$$\nh_{opt} = \\left(\\frac{4\\epsilon |y|}{|y^{\\prime\\prime}|}\\right)^\\frac{1}{2}\n$$\n\n:::\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nLets find the error of the following expression\n\n$$\ny^{\\prime\\prime} = ay(x-h) + by(x) + cy(x+h) + \\varepsilon\\\\\n$$\n\n\\begin{align*}\ny^{\\prime\\prime} &= a\\left(y(x)-hy^\\prime(x) + \\frac{h^2}{2}y^{\\prime\\prime}(x) - \\frac{h^3}{6}y^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}y^{(4)}(x) \\right)\\\\\n&+ by(x) \\\\\n&+ c\\left(y(x)+hy^\\prime(x) + \\frac{h^2}{2}y^{\\prime\\prime}(x) + \\frac{h^3}{6}y^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}y^{(4)}(x) \\right)\n\\end{align*}\n\nThus\n\n\\begin{align*}\n(a + b + c) = 0\\\\\nh(c-a) = 0\\\\\n\\frac{h^2}{2}(a-c) = 1\n\\end{align*}\n\nThus we have constants\n\n$$\nc = a\\quad a = \\frac{1}{h^2} \\quad b = -\\frac{2}{h^2}\n$$\n\nAnd we're left with the error\n\n\\begin{align*}\n\\varepsilon = -\\frac{h^2}{2y}\\left(y^{(4)}(c_1)+y^{(4)}(c_2)\\right)\n\\end{align*}\n\nThus we have\n\n$$\ny^{\\prime\\prime}(x) = \\frac{y(x-h)-2y(x)+y(x+h)}{h^2} + \\varepsilon\n$$\n\nLets find the optimal error. First we include the machine error\n\n$$\n\\epsilon_m = \\frac{4\\epsilon |y|}{h^2}\n$$\n\nCombine with our function error\n$$\n\\varepsilon_{T} = \\frac{4\\epsilon |y|}{h^2} +  \\frac{h^2}{2y}\\left(y^{(4)}\\right)\n$$\n\nContinue rest later.\n\n:::\n\nExample again\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nTaylor expand $f(x) = (1+x)^{1/2}$\n\nWe get clearly\n\n$$\nf^{(k)}(x) = \\frac{(-1)^{k+1}}{2^k}(3\\cdot 5\\cdots (2k-3))(1+x)^{\\frac{-2k-1}{2}}\n$$\n\nSo our taylor expansion\n\n$$\nf(x) = (1+x)^{1/2} = 1 + \\frac{x}{2} - \\frac{1}{2^2}\\frac{x^2}{2!} + \\frac{3}{2^3}\\frac{x^3}{3!} \\cdots + \\frac{-1^{k+1}}{2^k}(1\\cdot 3\\cdot 5\\cdots (2k-3))\\frac{x^k}{k!}\n$$\n:::\n\n## Newtons method\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nWrite the Newton Iteration to solve\n\n$$\nx = \\cos x\n$$\n\nNote that \n\n$$\nf(x) = x - \\cos x \n$$\n\n$$\nf^\\prime(x) = 1 + \\sin x\n$$\n\nSo\n\n$$\nx_{m+1} = x_m - \\frac{x_m - \\cos x_m}{1 + \\sin x_m}\n$$\n:::\n\nRemember that the error of Newtons method is approximatly equal to the multiplicity of the root\n\n$$\n\\varepsilon_{m+1} \\approxeq \\varepsilon_m^2 \\bigg\\lvert \\frac{f^{\\prime\\prime}(a)}{2f^\\prime(a)} \\bigg\\rvert\n$$\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nWrite the Newton Iteration to solve\n\n$$\nf(x) = x^\\alpha\\quad \\alpha>0\n$$\n\nWe want to write newtons iteration with an initial guess $x_0 \\not = 0$, and find for what values of $\\alpha$ does newtons iteration converge to root?\n\nFirst\n\n$$\nx_{m+1} x_m - \\frac{x^\\alpha_m}{\\alpha x^{\\alpha-1}_m} = x_m\\left(1-\\frac{1}{\\alpha}\\right)\n$$\n\nThis will converge if \n\n$$\n-1 < 1 - \\frac{1}{\\alpha} < 1\n$$\n\nThus\n\n$$\n\\frac{1}{\\alpha} < 2 \\implies \\alpha > \\frac{1}{2}\n$$\n\n:::\n\nProvided the multiplicity of the root is $1$, it converges quadratically. Otherwise the newton method converges linearly, with the rate equal to $S = \\frac{m-1}{m}$.\n\nLets assume you can't calculate the derivative. We instead use the approximation\n\n$$\nf^\\prime(x) \\approx \\frac{f(x + h)-f(x)}{h}\n$$\n\nOr we use the secant method.\n\n$$\nf^\\prime(x_m) \\approx \\frac{f(x_m) - f(x_{m-1})}{x_m - x_{m-1}}\n$$\n\nThis is useful when $f(x_m)$ is expensive to compute, but the disadvantage is the convergence is slightly slower.\n\n$$\n\\varepsilon_{m+1} \\approx \\varepsilon_m^\\alpha \\bigg\\lvert \\frac{f^{\\prime\\prime}(r)}{2f^\\prime(r)}\\bigg\\rvert\n$$\n\nCombination of Newton and Bisection\n\nFirst we draw a secant line from the points $a$ to $b$. This gives us an equation\n\n$$\ny = f(a) + (x - a)\\frac{f(b)-f(a)}{b-a}\n$$\n\nWhich gives us \n\n$$\nx = \\frac{a(f(b)-f(a))(b-a)}{f(b)-f(a)} = \\frac{af(b)-bf(a)}{f(b)-f(a)}\n$$\n\n## Interpolation\n\nGiven $m$ points $(x_1,y_1)$ through $(x_m,y_m)$, we want to find a continous function $P_{m-1}(x)$ such that for all $i = 1,\\ldots,m$\n\n$$\nP_{m-1}(x_i) = y_i\n$$\n\nWe can find this function through [**Lagrange interpolation**]{style=\"color: blue\"}.\n\nDefine\n\n$$\nL_k(x) = \\frac{(x-x_1)(x-x_2)\\cdots(x-x_{k-1})(x-x_{k+1})\\cdots(x-x_m)}{(x_k-x_1)(x_k-x_2)\\cdots(x_k - x_{k-1})(x_k - x_{k+1})\\cdots(x_k - x_m)}\n$$\n\nWe can clearly see that \n\n$$\nL_k(x_k) = 1,\\quad L_k(x_i) = 0\n$$\n\nAnd that $L_k$ is a polynomial of degree $m-1$. Here we can define\n\n$$\nP_{m-1}(x) = y_1L_1(x) + \\cdots + y_mL_m(x) \n$$\n\nAs a polynomial of degree $\\leq m-1$. This is called the [**Lagrange polynomial**]{style=\"color: blue\"}. This gives us a function $P_{m-1}(x)$ such that\n\n$$\nP_{m-1}(x_i) = y_1L_1(x_i) + \\cdots + y_mL_m(x_i) = y_i \n$$\n\nWhich is pretty cool. But even better, this has the special property. Assume there exists some *true* function $f(x)$ that we are interpolating at a finite amount of points through $P_{m-1}(x)$. Then we have the error\n\n$$\ny(x) - P_{m-1}(x) = \\frac{y^{(m)}(c)}{m!}(x-x_1)(x-x_2)\\cdots(x-x_m)\n$$\n\nWhere $c \\in (x_1,x_m)$. Note that we can make $m$ as large as we want by adding more points, and due to the $m!$ in the denominator, we can make our $P_{m-1}(x)$ accurate incredibly quickly.\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nInterpolate $y(x) = \\sin(x)$ using interpolation points\n\n$$\nx_1 = 0\\; x_2 = \\frac{\\pi}{2} \\; x_3 = \\pi\n$$\n\nAnd compute the error.\n\nWe can see\n\n$$\ny_1 = 0 \\; y_2 = 1 \\; y_3 = 0\n$$\n\nWe therefore want a polynomial of degree two that interpolates the function.\n\n$$\nP_2(x) = y_1L_1 + y_2L_2 + y_3L_3\n$$\n\n$$\nP_2(x) = (0)\\frac{(x-\\pi /2)(x-\\pi)}{(0 - \\pi/2)(0-\\pi)} + (1)\\frac{(x-0)(x-\\pi)}{(\\pi /2 - 0)(\\pi /2 - \\pi)} + (0)\\frac{(x-0)(x- \\pi /2)}{(\\pi - 0) (\\pi - \\pi / 2)}\n$$\n\n$$\nP_2(x) = -x(x-\\pi)\\frac{4}{\\pi^2}\n$$\n\nThus we have a function that approximates $\\sin(x)$ using a parabola. Now we need to compute the error.\n\n$$\n\\varepsilon = \\frac{f^{(m)}(c)}{m!}(x-x_1)\\cdots(x-x_m)\n$$\n\n$$\n\\varepsilon = \\frac{-\\cos(c)}{6}(x-0)\\left(x-\\frac{\\pi}{2}\\right)(x-\\pi)\n$$\n\nTaking the absolute value, and the maximum value of cosine, we get our error\n$$\n|\\varepsilon| \\leq \\frac{1}{6}|x\\left(x-\\frac{\\pi}{2}\\right)(x-\\pi)|\n$$\n\nWe can find the maximum error taking the derivative of $\\varepsilon$\n\n$$\n\\varepsilon^{\\prime} = 3x^2 + 3\\pi x + \\frac{\\pi^2}{2} = 0\n$$\n\nWe get\n\n$$\nx_{\\text{max}} = \\frac{-3\\pi \\pm \\pi \\sqrt{3}}{6}\n$$\n\n:::\n\nDefine the function\n\n$$\nh(x) = y(x) - P_{m-1}(x) - c(x-x_i)(x-x_2)\\cdots(x-x_m)\n$$\n\nThis has the property that for any $i \\in 1,\\ldots,m$\n\n$$\nh(x_i) = 0\n$$\n\nWe can choose a constant $c$ such that for any $x^* \\in (x_1,x_m)$\n\n$$\nh(x^*) = 0\n$$\n\nThis gives us a function with $m+1$ zeros, specifically a polynomial of degree $m+1$. We can use Rolle's theorem in order to get a function $h^\\prime(x)$ which has $m$ roots. We can repeat this process until $h^{(m)}(x)$ has one zero. That being\n\n$$\nh^{(m)}(c) = 0 = y^{(m)}(c) - 0 - C(m!)\n$$\n\nThus\n\n$$\nC = \\frac{y^{(m)}(c)}{m!}\n$$\n\nThat directly leads to our error formula\n\n$$\ny(x) - P_{m-1}(x) = \\frac{y^{(m)}(c)}{m!}(x-x_1)(x-x_2)\\cdots(x-x_m)\n$$\n\nYou can choose specific $x_1$,$x_2$,$x_3$ in order to make the error as small as possible.\n\n## Error\n\nSuppose that we have an error \n\n$$\n\\varepsilon = \\frac{h^3}{4}f^{(4)}(c_1) + \\frac{h^3}{8}f^{\\prime\\prime}(c_2)\n$$\n\nWe have a simplified expression\n\n$$\n\\varepsilon = h^3(\\frac{1}{4}+\\frac{1}{8})f^{\\prime\\prime}(c)\n$$\n\nFor some $c$.\n\n## Multi-panel integration formula\n\n$$\n\\int\\limits^b_a f(s) ds = \\int\\limits^{a+h}_a f(s) ds + \\int\\limits^{a+2h}_{a+h} f(s) ds \\cdots \\int\\limits^{b}_{b-h} f(s) ds\n$$\n\nUsing the formula from last class, this is equal too\n\n$$\n\\int\\limits_a^{a+h}f(s)ds = \\frac{h}{2}(f(a)+f(a+h))-\\frac{h^3}{12}\nf^{\\prime\\prime}(c_1)\n$$\n\nExpanding our initial integral, we can see it's telescoping. This gives\n\n$$\n\\int\\limits^b_a f(s) ds = \\frac{h}{2}\\left(f(a)+2f(a+h)\\cdots2f(b-h) + f(b)\\right) - \\varepsilon\n$$\n\n$$\n\\varepsilon = \\frac{h^3}{12}(f^{\\prime\\prime}(c_1)\\cdots f^{\\prime\\prime}(c_m))\n$$\n\nUsing the generalized intermediate value theorem, we can simplify\n\n$$\n\\varepsilon= \\frac{h^3}{12}mf^{\\prime\\prime}(c) = \\frac{(a-b)h^2}{12}f^{\\prime\\prime}(c)\n$$\n\nWhich gives us an error for the trapezoidal formula.\n\nfor what m is the error less than $10^{-6}$/\n\n## Simpson integration\n\n$$\n\\mathrlap{\\int\\limits_x^{x+2h}}\\quad\\quad f(s) \\,ds = \\frac{h^3}{3}(f(x) + 4f(x+h) + f(x+2h)) - \\frac{h^5}{90}f^{(4)}(c)\n$$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n## Floating point\n\nComputers don't use exact numbers when preforming calculations, but instead use a floating point system. \n\n::: {.callout-note icon=\"false\"}\n## Definition\n\nA [**floating point**]{style=\"color: blue\"} number $x$ is expressed as \n\n$$\nx = S \\times 2^E \\times 1.b_1b_2\\ldots b_{52}\n$$\n\nSuch that $S$ represents the sign of the number, $+$ or $-$. $E$ is an exponent, and $b_1$ through $b_52$ are binary digits.\n\n:::\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nSuppose we have a variable $x = 6$ and we wish to express it as a floating point number.  We first convert the number to binary\n\n$$\n6_{10} = 110_{2}\n$$\n\nWe now specify where the decimal is, by adding an exponent \n\n$$\n6_{10} = 110_{2} = 2^2\\times1.10_2\n$$\n\nAnd finally adding the sign\n\n$$\n6_{10} = 110_{2} = 1\\times2^2\\times1.10_2\n$$\n\n:::\n\nWe can also express fractions, particular ones without a terminating digit. For example,\n\n$$\n0.3 = 1 \\times 2^{-2} \\times 1.00110011001\\ldots\n$$\n\nTerminating after $52$ digits.\n\n### Loss of signifigance\n\nComputers themselves store numbers as a collection of $64$ bits\n\n$$\nx = s_1e_1\\ldots e_{11}m_1\\ldots m_{52}\n$$\n\nOne bit for the sign of a number, $11$ bits for the exponent, and $52$ bits for the mantissa (digits after the decimal place).\n\nClearly this means that only a finite amount of numbers can be expressed in floating point, and numbers are rounded to their nearest corresponding floating point. This leads to a [**loss of signifigance**]{style=\"color: blue\"}, and means functions with extremly large or small inputs will be catastrophically wrong, as the difference between two numbers is completly outside of the range floating point can represent.\n\nTake the identical functions $f(x)$ and $g(x)$\n\n\n$$\nf(x) = \\frac{1 - \\cos(x)}{\\sin^2(x)}\n$$\n\n$$\ng(x) = \\frac{1}{1 + \\cos(x)} \n$$\n\n$f(x)$ suffers from an egregious loss of signifigance error as $x$ decreases towards $0$, But $g(x)$ won't.\n\n### Rounding\n\nThe number $101.101$ rounds up to $110$, because the digits after the decimal are above $.1$. If they aren't the number terminates at the decimal. If $101.1\\overline{0}$ then we get $110$, but if we have $110.1\\overline{0}$, we drop the number. This is based on the digit in the units place. If it is a $1$, we round up, if it is not, we drop it.\n\n## Taylor series\n\nThe [**taylor expansion**]{style=\"color: blue\"} of a function $f(x)$ is given by\n\n$$\nf(x) = \\sum^m_{k=0} \\frac{f^{(k)}(x)(x-x_0)^k}{k!} + \\frac{f^{(m + 1)}(c)(x-x_0)^{m+1}}{(m+1)!}\n$$\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nTake the function $f(x) = e^x$ We can taylor expand it to get\n\n$$\nf(x) = e^x = 1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\cdots + \\frac{x^k}{k!} + \\frac{e^cx^{k+1}}{k+1!}\n$$\n:::\n\nWe can also write the Taylor series\n\n$$\nf(x + h) = \\sum^m_{k=0} \\frac{h^kf^{(k)}(x)}{k!}\n$$\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nFind the error\n\n\\begin{align*}\ny^\\prime(x) &= ay(x) + by(x + h) + \\varepsilon\\\\\ny^\\prime(x) &= ay(x) + b\\left(y(x) - hy^\\prime(x) + \\frac{h^2}{2}y^{\\prime\\prime}(c_1)\\right) + \\varepsilon\\\\\ny^\\prime(x) &= (a+b)y(x) - bhy^\\prime(x) + \\frac{bh^2}{2}y^{\\prime\\prime}(c_1) + \\varepsilon\\\\\n\\end{align*}\n\nSo \n\n$$\na = -b\\quad b = -\\frac{1}{h}\\quad \n$$\n\nThus we get\n\n$$\ny^\\prime(x) = \\frac{y(x+h) - y(x)}{h} + \\varepsilon\n$$\n\nwith an error\n$$\n\\varepsilon = -\\frac{h}{2}y^{\\prime\\prime}(c_1).\n$$\n:::\n\nWhen we plug these numbers into the computer however, we are going to have an error as we have only $52$ digits of precision. This gives us the machine error $\\epsilon$. So if we wanted to numerically approximate the derivative\n\n$$\ny^\\prime(x) = \\frac{y(x+h) - y(x) + 2\\epsilon y}{h} + \\varepsilon\n$$\n\nThis gives us the total error, the sum of the machine error, and the taylor error.\n\n$$\n\\varepsilon_T = \\epsilon + \\varepsilon\n$$\n\n$$\n\\varepsilon_T = \\frac{2\\epsilon |y|}{2h} + \\frac{2h^2}{12}|y^{\\prime\\prime\\prime}(x)|\n$$\n\nBy computing the derivative, we can find the smallest total error.\n\n$$\n\\frac{d\\varepsilon_T}{dh} = -\\frac{1}{h^2}\\epsilon |y| + \\frac{h}{3}|y^{\\prime\\prime\\prime}| = 0\n$$\n\nSolving for $h$\n\n$$\nh_{opt} =\\left(\\frac{3\\epsilon |y|}{|y^{\\prime\\prime\\prime}|}\\right)^\\frac{1}{3}\n$$\n\nFor example, lets try the function $y = e^x$. This gives us an optimal error\n\n$$\nh_{opt} = \\left( 3 \\times 2^{-52} \\frac{e^x}{e^x} \\right)^\\frac{1}{3}\n$$\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nLets find the optimal value of $h$ using a different error.\n\n$$\n\\varepsilon = -\\frac{h}{2}y^{\\prime\\prime}(c_1)\n$$\n\nWe can rewrite our equation\n\n$$\ny^\\prime(x) = \\frac{y(x+h) - y(x) + 2\\epsilon y}{h} + -\\frac{h}{2}y^{\\prime\\prime}(c_1)\n$$\n\nThen\n\n$$\n\\varepsilon_T = \\frac{2\\epsilon |y|}{h} + \\frac{h}{2}y^{\\prime\\prime}\n$$\n\nTaking the derivative\n\n$$\n\\frac{d \\varepsilon_T}{dh} = -\\frac{2\\epsilon |y|}{h^2} + \\frac{1}{2}y^{\\prime\\prime} = 0\n$$\n\nWe get\n\n$$\n\\frac{2\\epsilon |y|}{h^2} = \\frac{1}{2}y^{\\prime\\prime}\n$$\n\nThus\n\n$$\nh_{opt} = \\left(\\frac{4\\epsilon |y|}{|y^{\\prime\\prime}|}\\right)^\\frac{1}{2}\n$$\n\n:::\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nLets find the error of the following expression\n\n$$\ny^{\\prime\\prime} = ay(x-h) + by(x) + cy(x+h) + \\varepsilon\\\\\n$$\n\n\\begin{align*}\ny^{\\prime\\prime} &= a\\left(y(x)-hy^\\prime(x) + \\frac{h^2}{2}y^{\\prime\\prime}(x) - \\frac{h^3}{6}y^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}y^{(4)}(x) \\right)\\\\\n&+ by(x) \\\\\n&+ c\\left(y(x)+hy^\\prime(x) + \\frac{h^2}{2}y^{\\prime\\prime}(x) + \\frac{h^3}{6}y^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}y^{(4)}(x) \\right)\n\\end{align*}\n\nThus\n\n\\begin{align*}\n(a + b + c) = 0\\\\\nh(c-a) = 0\\\\\n\\frac{h^2}{2}(a-c) = 1\n\\end{align*}\n\nThus we have constants\n\n$$\nc = a\\quad a = \\frac{1}{h^2} \\quad b = -\\frac{2}{h^2}\n$$\n\nAnd we're left with the error\n\n\\begin{align*}\n\\varepsilon = -\\frac{h^2}{2y}\\left(y^{(4)}(c_1)+y^{(4)}(c_2)\\right)\n\\end{align*}\n\nThus we have\n\n$$\ny^{\\prime\\prime}(x) = \\frac{y(x-h)-2y(x)+y(x+h)}{h^2} + \\varepsilon\n$$\n\nLets find the optimal error. First we include the machine error\n\n$$\n\\epsilon_m = \\frac{4\\epsilon |y|}{h^2}\n$$\n\nCombine with our function error\n$$\n\\varepsilon_{T} = \\frac{4\\epsilon |y|}{h^2} +  \\frac{h^2}{2y}\\left(y^{(4)}\\right)\n$$\n\nContinue rest later.\n\n:::\n\nExample again\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nTaylor expand $f(x) = (1+x)^{1/2}$\n\nWe get clearly\n\n$$\nf^{(k)}(x) = \\frac{(-1)^{k+1}}{2^k}(3\\cdot 5\\cdots (2k-3))(1+x)^{\\frac{-2k-1}{2}}\n$$\n\nSo our taylor expansion\n\n$$\nf(x) = (1+x)^{1/2} = 1 + \\frac{x}{2} - \\frac{1}{2^2}\\frac{x^2}{2!} + \\frac{3}{2^3}\\frac{x^3}{3!} \\cdots + \\frac{-1^{k+1}}{2^k}(1\\cdot 3\\cdot 5\\cdots (2k-3))\\frac{x^k}{k!}\n$$\n:::\n\n## Newtons method\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nWrite the Newton Iteration to solve\n\n$$\nx = \\cos x\n$$\n\nNote that \n\n$$\nf(x) = x - \\cos x \n$$\n\n$$\nf^\\prime(x) = 1 + \\sin x\n$$\n\nSo\n\n$$\nx_{m+1} = x_m - \\frac{x_m - \\cos x_m}{1 + \\sin x_m}\n$$\n:::\n\nRemember that the error of Newtons method is approximatly equal to the multiplicity of the root\n\n$$\n\\varepsilon_{m+1} \\approxeq \\varepsilon_m^2 \\bigg\\lvert \\frac{f^{\\prime\\prime}(a)}{2f^\\prime(a)} \\bigg\\rvert\n$$\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nWrite the Newton Iteration to solve\n\n$$\nf(x) = x^\\alpha\\quad \\alpha>0\n$$\n\nWe want to write newtons iteration with an initial guess $x_0 \\not = 0$, and find for what values of $\\alpha$ does newtons iteration converge to root?\n\nFirst\n\n$$\nx_{m+1} x_m - \\frac{x^\\alpha_m}{\\alpha x^{\\alpha-1}_m} = x_m\\left(1-\\frac{1}{\\alpha}\\right)\n$$\n\nThis will converge if \n\n$$\n-1 < 1 - \\frac{1}{\\alpha} < 1\n$$\n\nThus\n\n$$\n\\frac{1}{\\alpha} < 2 \\implies \\alpha > \\frac{1}{2}\n$$\n\n:::\n\nProvided the multiplicity of the root is $1$, it converges quadratically. Otherwise the newton method converges linearly, with the rate equal to $S = \\frac{m-1}{m}$.\n\nLets assume you can't calculate the derivative. We instead use the approximation\n\n$$\nf^\\prime(x) \\approx \\frac{f(x + h)-f(x)}{h}\n$$\n\nOr we use the secant method.\n\n$$\nf^\\prime(x_m) \\approx \\frac{f(x_m) - f(x_{m-1})}{x_m - x_{m-1}}\n$$\n\nThis is useful when $f(x_m)$ is expensive to compute, but the disadvantage is the convergence is slightly slower.\n\n$$\n\\varepsilon_{m+1} \\approx \\varepsilon_m^\\alpha \\bigg\\lvert \\frac{f^{\\prime\\prime}(r)}{2f^\\prime(r)}\\bigg\\rvert\n$$\n\nCombination of Newton and Bisection\n\nFirst we draw a secant line from the points $a$ to $b$. This gives us an equation\n\n$$\ny = f(a) + (x - a)\\frac{f(b)-f(a)}{b-a}\n$$\n\nWhich gives us \n\n$$\nx = \\frac{a(f(b)-f(a))(b-a)}{f(b)-f(a)} = \\frac{af(b)-bf(a)}{f(b)-f(a)}\n$$\n\n## Interpolation\n\nGiven $m$ points $(x_1,y_1)$ through $(x_m,y_m)$, we want to find a continous function $P_{m-1}(x)$ such that for all $i = 1,\\ldots,m$\n\n$$\nP_{m-1}(x_i) = y_i\n$$\n\nWe can find this function through [**Lagrange interpolation**]{style=\"color: blue\"}.\n\nDefine\n\n$$\nL_k(x) = \\frac{(x-x_1)(x-x_2)\\cdots(x-x_{k-1})(x-x_{k+1})\\cdots(x-x_m)}{(x_k-x_1)(x_k-x_2)\\cdots(x_k - x_{k-1})(x_k - x_{k+1})\\cdots(x_k - x_m)}\n$$\n\nWe can clearly see that \n\n$$\nL_k(x_k) = 1,\\quad L_k(x_i) = 0\n$$\n\nAnd that $L_k$ is a polynomial of degree $m-1$. Here we can define\n\n$$\nP_{m-1}(x) = y_1L_1(x) + \\cdots + y_mL_m(x) \n$$\n\nAs a polynomial of degree $\\leq m-1$. This is called the [**Lagrange polynomial**]{style=\"color: blue\"}. This gives us a function $P_{m-1}(x)$ such that\n\n$$\nP_{m-1}(x_i) = y_1L_1(x_i) + \\cdots + y_mL_m(x_i) = y_i \n$$\n\nWhich is pretty cool. But even better, this has the special property. Assume there exists some *true* function $f(x)$ that we are interpolating at a finite amount of points through $P_{m-1}(x)$. Then we have the error\n\n$$\ny(x) - P_{m-1}(x) = \\frac{y^{(m)}(c)}{m!}(x-x_1)(x-x_2)\\cdots(x-x_m)\n$$\n\nWhere $c \\in (x_1,x_m)$. Note that we can make $m$ as large as we want by adding more points, and due to the $m!$ in the denominator, we can make our $P_{m-1}(x)$ accurate incredibly quickly.\n\n::: {.callout-warning icon=\"false\"}\n## Example\n\nInterpolate $y(x) = \\sin(x)$ using interpolation points\n\n$$\nx_1 = 0\\; x_2 = \\frac{\\pi}{2} \\; x_3 = \\pi\n$$\n\nAnd compute the error.\n\nWe can see\n\n$$\ny_1 = 0 \\; y_2 = 1 \\; y_3 = 0\n$$\n\nWe therefore want a polynomial of degree two that interpolates the function.\n\n$$\nP_2(x) = y_1L_1 + y_2L_2 + y_3L_3\n$$\n\n$$\nP_2(x) = (0)\\frac{(x-\\pi /2)(x-\\pi)}{(0 - \\pi/2)(0-\\pi)} + (1)\\frac{(x-0)(x-\\pi)}{(\\pi /2 - 0)(\\pi /2 - \\pi)} + (0)\\frac{(x-0)(x- \\pi /2)}{(\\pi - 0) (\\pi - \\pi / 2)}\n$$\n\n$$\nP_2(x) = -x(x-\\pi)\\frac{4}{\\pi^2}\n$$\n\nThus we have a function that approximates $\\sin(x)$ using a parabola. Now we need to compute the error.\n\n$$\n\\varepsilon = \\frac{f^{(m)}(c)}{m!}(x-x_1)\\cdots(x-x_m)\n$$\n\n$$\n\\varepsilon = \\frac{-\\cos(c)}{6}(x-0)\\left(x-\\frac{\\pi}{2}\\right)(x-\\pi)\n$$\n\nTaking the absolute value, and the maximum value of cosine, we get our error\n$$\n|\\varepsilon| \\leq \\frac{1}{6}|x\\left(x-\\frac{\\pi}{2}\\right)(x-\\pi)|\n$$\n\nWe can find the maximum error taking the derivative of $\\varepsilon$\n\n$$\n\\varepsilon^{\\prime} = 3x^2 + 3\\pi x + \\frac{\\pi^2}{2} = 0\n$$\n\nWe get\n\n$$\nx_{\\text{max}} = \\frac{-3\\pi \\pm \\pi \\sqrt{3}}{6}\n$$\n\n:::\n\nDefine the function\n\n$$\nh(x) = y(x) - P_{m-1}(x) - c(x-x_i)(x-x_2)\\cdots(x-x_m)\n$$\n\nThis has the property that for any $i \\in 1,\\ldots,m$\n\n$$\nh(x_i) = 0\n$$\n\nWe can choose a constant $c$ such that for any $x^* \\in (x_1,x_m)$\n\n$$\nh(x^*) = 0\n$$\n\nThis gives us a function with $m+1$ zeros, specifically a polynomial of degree $m+1$. We can use Rolle's theorem in order to get a function $h^\\prime(x)$ which has $m$ roots. We can repeat this process until $h^{(m)}(x)$ has one zero. That being\n\n$$\nh^{(m)}(c) = 0 = y^{(m)}(c) - 0 - C(m!)\n$$\n\nThus\n\n$$\nC = \\frac{y^{(m)}(c)}{m!}\n$$\n\nThat directly leads to our error formula\n\n$$\ny(x) - P_{m-1}(x) = \\frac{y^{(m)}(c)}{m!}(x-x_1)(x-x_2)\\cdots(x-x_m)\n$$\n\nYou can choose specific $x_1$,$x_2$,$x_3$ in order to make the error as small as possible.\n\n## Error\n\nSuppose that we have an error \n\n$$\n\\varepsilon = \\frac{h^3}{4}f^{(4)}(c_1) + \\frac{h^3}{8}f^{\\prime\\prime}(c_2)\n$$\n\nWe have a simplified expression\n\n$$\n\\varepsilon = h^3(\\frac{1}{4}+\\frac{1}{8})f^{\\prime\\prime}(c)\n$$\n\nFor some $c$.\n\n## Multi-panel integration formula\n\n$$\n\\int\\limits^b_a f(s) ds = \\int\\limits^{a+h}_a f(s) ds + \\int\\limits^{a+2h}_{a+h} f(s) ds \\cdots \\int\\limits^{b}_{b-h} f(s) ds\n$$\n\nUsing the formula from last class, this is equal too\n\n$$\n\\int\\limits_a^{a+h}f(s)ds = \\frac{h}{2}(f(a)+f(a+h))-\\frac{h^3}{12}\nf^{\\prime\\prime}(c_1)\n$$\n\nExpanding our initial integral, we can see it's telescoping. This gives\n\n$$\n\\int\\limits^b_a f(s) ds = \\frac{h}{2}\\left(f(a)+2f(a+h)\\cdots2f(b-h) + f(b)\\right) - \\varepsilon\n$$\n\n$$\n\\varepsilon = \\frac{h^3}{12}(f^{\\prime\\prime}(c_1)\\cdots f^{\\prime\\prime}(c_m))\n$$\n\nUsing the generalized intermediate value theorem, we can simplify\n\n$$\n\\varepsilon= \\frac{h^3}{12}mf^{\\prime\\prime}(c) = \\frac{(a-b)h^2}{12}f^{\\prime\\prime}(c)\n$$\n\nWhich gives us an error for the trapezoidal formula.\n\nfor what m is the error less than $10^{-6}$/\n\n## Simpson integration\n\n$$\n\\mathrlap{\\int\\limits_x^{x+2h}}\\quad\\quad f(s) \\,ds = \\frac{h^3}{3}(f(x) + 4f(x+h) + f(x+2h)) - \\frac{h^5}{90}f^{(4)}(c)\n$$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":"katex","output-file":"fundementals.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","title":"Fundementals"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}